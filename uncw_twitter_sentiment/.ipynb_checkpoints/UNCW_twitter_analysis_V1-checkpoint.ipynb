{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import webbrowser\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup Twitter API client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get started here: https://developer.twitter.com/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'your consumer key here'\n",
    "consumer_secret = 'your consumer secret here'\n",
    "\n",
    "access_token = 'your access token here'\n",
    "access_token_secret = 'your access token secret here'\n",
    "\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "   consumer_key, consumer_secret, access_token, access_token_secret\n",
    ")\n",
    "\n",
    "client = tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your connection\n",
    "public_tweets = client.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize your string with nltk\n",
    "def tokenize_text(text: str):\n",
    "    \n",
    "    # import needed packages\n",
    "    import nltk\n",
    "    import re\n",
    "    \n",
    "    # remove unwanted new line and tab characters from the text\n",
    "    for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
    "        text = text.replace(char, \" \")\n",
    "    \n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation from text\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove stopwords from txt_tokens and word_tokens\n",
    "    from nltk.corpus import stopwords\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in english_stop_words]\n",
    "    \n",
    "    # return your tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize your tokens with nltk\n",
    "def lemmatize_tokens(tokens):\n",
    "    \n",
    "    # import needed packages\n",
    "    import nltk\n",
    "    nltk.download('wordnet')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # return your lemmatized tokens\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the most common tokens\n",
    "def return_top_tokens(tokens,\n",
    "                      top_N = 10):\n",
    "    \n",
    "    import nltk\n",
    "\n",
    "    # first, count the frequency of every unique token\n",
    "    word_token_distribution = nltk.FreqDist(tokens)\n",
    "    \n",
    "    # next, filter for only the most common top_N tokens\n",
    "    # also, put this in a dataframe\n",
    "    top_tokens = pd.DataFrame(word_token_distribution.most_common(top_N),\n",
    "                              columns=['Word', 'Frequency'])\n",
    "    \n",
    "    # return the top_tokens dataframe\n",
    "    return top_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the most common bi-grams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "def return_top_bigrams(tokens,\n",
    "                       top_N = 10):\n",
    "    \n",
    "    # collect bigrams\n",
    "    bcf = BigramCollocationFinder.from_words(tokens)\n",
    "    \n",
    "    # put bigrams into a dataframe\n",
    "    bigram_df = pd.DataFrame(data = bcf.ngram_fd.items(),\n",
    "                             columns = ['Bigram', 'Frequency'])\n",
    "    \n",
    "    # sort the dataframe by frequency\n",
    "    bigram_df = bigram_df.sort_values(by=['Frequency'],ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    # filter for only top bigrams\n",
    "    bigram_df = bigram_df[0:top_N]\n",
    "    \n",
    "    # return the bigram dataframe\n",
    "    return bigram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def return_sentiment_df(tokens):\n",
    "\n",
    "    # initialize sentiment analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # create some counters for sentiment of each token\n",
    "    positive_tokens = 0\n",
    "    negative_tokens = 0\n",
    "    neutral_tokens = 0\n",
    "    compound_scores = []\n",
    "        \n",
    "    # loop through each token\n",
    "    for token in tokens:\n",
    "        \n",
    "        if sia.polarity_scores(token)[\"compound\"] > 0:\n",
    "            \n",
    "            positive_tokens += 1\n",
    "            compound_scores.append(sia.polarity_scores(token)[\"compound\"])\n",
    "            \n",
    "        elif sia.polarity_scores(token)[\"compound\"] < 0:\n",
    "            \n",
    "            negative_tokens += 1\n",
    "            compound_scores.append(sia.polarity_scores(token)[\"compound\"])\n",
    "              \n",
    "        elif sia.polarity_scores(token)[\"compound\"] == 0:\n",
    "            \n",
    "            neutral_tokens += 1\n",
    "            compound_scores.append(sia.polarity_scores(token)[\"compound\"])\n",
    "            \n",
    "    # put sentiment results into a dataframe\n",
    "    compound_score_numbers = [num for num in compound_scores if num != 0]\n",
    "    sentiment_df = pd.DataFrame(data = {\"total_tokens\" : len(tokens),\n",
    "                                        \"positive_tokens\" : positive_tokens,\n",
    "                                        \"negative_tokens\" : negative_tokens,\n",
    "                                        \"neutral_tokens\" : neutral_tokens,\n",
    "                                        \"compound_sentiment_score\" : sum(compound_score_numbers) / len(compound_score_numbers)},\n",
    "                                index = [0])\n",
    "\n",
    "    # return sentiment_df\n",
    "    return sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultimate_wordcloud_function(text: str,\n",
    "                                output_filepath: str,\n",
    "                                mask_path = None,\n",
    "                                white_mask_background = True,\n",
    "                                width = 725,\n",
    "                                height = 300,\n",
    "                                background_color = \"white\",\n",
    "                                colormap = \"viridis\",\n",
    "                                contour_color = \"steelblue\",\n",
    "                                contour_width = 3,\n",
    "                                collocations = False,\n",
    "                                max_words = 2000,\n",
    "                                max_font_size = 40,\n",
    "                                min_font_size = 4,\n",
    "                                prefer_horizontal = 0.9,\n",
    "                                include_numbers = True):\n",
    "    \n",
    "    # start function timer\n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    # tokenize and lemmatize your text\n",
    "    tokens = tokenize_text(text = text)\n",
    "    lemmatized_tokens = lemmatize_tokens(tokens = tokens)\n",
    "    \n",
    "    # import needed packages\n",
    "    from wordcloud import WordCloud\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # create a wordcloud object without a mask\n",
    "    if mask_path == None:\n",
    "    \n",
    "        # create a WordCloud object\n",
    "        wordcloud = WordCloud(width = width,\n",
    "                              height = height,\n",
    "                              background_color = background_color,\n",
    "                              colormap = colormap,\n",
    "                              collocations = collocations,\n",
    "                              max_words = max_words,\n",
    "                              max_font_size = max_font_size,\n",
    "                              min_font_size = min_font_size,\n",
    "                              prefer_horizontal = prefer_horizontal,\n",
    "                              include_numbers = include_numbers)\n",
    "    \n",
    "    # create a wordcloud object with a mask image\n",
    "    elif mask_path != None:\n",
    "        \n",
    "        # open the mask image as a numpy array\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        \n",
    "        # if your mask has a black background update to white\n",
    "        if white_mask_background == False:\n",
    "            mask[mask[:, :] == 0] = 255\n",
    "        \n",
    "        # create a WordCloud object\n",
    "        wordcloud = WordCloud(mask = mask,\n",
    "                              width=mask.shape[1],\n",
    "                              height=mask.shape[0],\n",
    "                              background_color = background_color,\n",
    "                              colormap = colormap,\n",
    "                              contour_color = contour_color,\n",
    "                              contour_width = contour_width,\n",
    "                              collocations = collocations,\n",
    "                              max_words = max_words,\n",
    "                              max_font_size = max_font_size,\n",
    "                              min_font_size = min_font_size,\n",
    "                              prefer_horizontal = prefer_horizontal,\n",
    "                              include_numbers = include_numbers)\n",
    "\n",
    "    # generate a word cloud (must join the tokens into a string)\n",
    "    wordcloud.generate(','.join(lemmatized_tokens))\n",
    "\n",
    "    # end wordcloud timer\n",
    "    end = time.time()\n",
    "    print(f\"wordcloud created in {round(end-start, 1)} seconds\")\n",
    "    \n",
    "    # print, save, and return the wordcloud\n",
    "    plt.imshow(wordcloud)\n",
    "    wordcloud.to_file(output_filepath)\n",
    "    return wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fall 2022 UNCW tweets analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Fall 2022 uncw tweets (without retweets)\n",
    "subject = \"uncw\"\n",
    "query_subject = f\"#{subject} -RT\"\n",
    "fromDate = 202208101600\n",
    "toDate = 202211121600\n",
    "number_of_tweets = 10000\n",
    "\n",
    "fall_tweets = tweepy.Cursor(client.search_full_archive,\n",
    "                       label = \"development\",\n",
    "                       query = query_subject,\n",
    "                       fromDate = fromDate,\n",
    "                       expansions=author_id,\n",
    "                       toDate = toDate).items(number_of_tweets)\n",
    "\n",
    "all_fall_tweets = []\n",
    "\n",
    "for tweet in fall_tweets:\n",
    "    all_fall_tweets.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of tweets: {len(all_fall_tweets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn list into a string\n",
    "fall_tweets_string = ''.join(str(x) for x in all_fall_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize tweets\n",
    "fall_tokens = tokenize_text(text = fall_tweets_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lematize tweets\n",
    "fall_lemmatized_tokens = lemmatize_tokens(tokens = fall_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top tokens\n",
    "fall_top_tokens = return_top_tokens(tokens = fall_lemmatized_tokens,\n",
    "                                    top_N = 15)\n",
    "print(fall_top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top bigrams\n",
    "fall_bigram_df = return_top_bigrams(tokens = fall_lemmatized_tokens,\n",
    "                                    top_N = 10)\n",
    "print(fall_bigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see tweets sentiment\n",
    "fall_sentiment_df = return_sentiment_df(tokens = fall_lemmatized_tokens)\n",
    "print(fall_sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and save wordcloud\n",
    "fall_uncw_wordcloud = ultimate_wordcloud_function(text = fall_tweets_string,\n",
    "                                                output_filepath = r\"YOUR FILEPATH HERE .png\",\n",
    "                                                width = 190,\n",
    "                                                height = 220,\n",
    "                                                background_color = \"white\",\n",
    "                                                colormap = \"viridis\",\n",
    "                                                collocations = False,\n",
    "                                                max_words = 700,\n",
    "                                                max_font_size = 40,\n",
    "                                                min_font_size = 4,\n",
    "                                                prefer_horizontal = 0.9,\n",
    "                                                include_numbers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at tweets regarding \"online\" and \"winter\"\n",
    "[tweet for tweet in fall_tweets if \"online\" in tweet.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tweet for tweet in fall_tweets if \"winter\" in tweet.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spring 2022 UNCW tweets analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Spring 2022 uncw tweets (without retweets)\n",
    "subject = \"uncw\"\n",
    "query_subject = f\"#{subject} -RT\"\n",
    "fromDate = 202201121600\n",
    "toDate = 202205161600\n",
    "number_of_tweets = 10000\n",
    "\n",
    "spring_tweets = tweepy.Cursor(client.search_full_archive,\n",
    "                       label = \"development\",\n",
    "                       query = query_subject,\n",
    "                       fromDate = fromDate,\n",
    "                       expansions=author_id,\n",
    "                       toDate = toDate).items(number_of_tweets)\n",
    "\n",
    "all_spring_tweets = []\n",
    "\n",
    "for tweet in spring_tweets:\n",
    "    all_spring_tweets.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of tweets: {len(all_spring_tweets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn list into a string\n",
    "spring_tweets_string = ''.join(str(x) for x in all_spring_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize tweets\n",
    "spring_tokens = tokenize_text(text = spring_tweets_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lematize tweets\n",
    "spring_lemmatized_tokens = lemmatize_tokens(tokens = spring_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top tokens\n",
    "spring_top_tokens = return_top_tokens(tokens = spring_lemmatized_tokens,\n",
    "                                    top_N = 15)\n",
    "print(spring_top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top bigrams\n",
    "spring_bigram_df = return_top_bigrams(tokens = spring_lemmatized_tokens,\n",
    "                                    top_N = 10)\n",
    "print(spring_bigram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see tweets sentiment\n",
    "spring_sentiment_df = return_sentiment_df(tokens = spring_lemmatized_tokens)\n",
    "print(spring_sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and save wordcloud\n",
    "spring_uncw_wordcloud = ultimate_wordcloud_function(text = spring_tweets_string,\n",
    "                                                output_filepath = r\"YOUR FILEPATH HERE .png\",\n",
    "                                                width = 190,\n",
    "                                                height = 220,\n",
    "                                                background_color = \"white\",\n",
    "                                                colormap = \"viridis\",\n",
    "                                                collocations = False,\n",
    "                                                max_words = 700,\n",
    "                                                max_font_size = 40,\n",
    "                                                min_font_size = 4,\n",
    "                                                prefer_horizontal = 0.9,\n",
    "                                                include_numbers = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
